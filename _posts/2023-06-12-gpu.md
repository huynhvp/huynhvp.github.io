---
layout: post
title: Experimental benchmarks on the GPU requirements for the training/fine-tuning of LLMs.
date: 2023-06-12 10:09:00
description: 
tags: dev
categories: language_model, gpu
---

---

This post gathers experimental setups on the GPU usage for the training/fine-tuning of LLMs in the wild.

---

<b>Table of Contents</b>
* TOC []
{:toc}


###### <b>Fine-tuning with DeepSpeed</b>

- <b>FLAN-XL (3B) + FLAN-XL (11B)</b>

    ![](/assets/img/gpu/ft_deepspeed_flan.png){:style="width: 50%; display:block; margin-left:auto; margin-right:auto"} *(Source: https://www.philschmid.de/fine-tune-flan-t5-deepspeed)*


###### <b>Fine-tuning with FSDP</b>

- <b>GPT-2 XL (1.5B)</b>

    2x24GB NVIDIA RTX

    ![](/assets/img/gpu/fsdp_gpt.png){:style="width: 50%; display:block; margin-left:auto; margin-right:auto"} *(Source: https://www.philschmid.de/fine-tune-flan-t5-deepspeed)*

###### <b>Fine-tuning with LoRA</b>

- <b>TO (3B) + mTO (12B) + BLOOMZ (7B) (7B)</b>

    ![](/assets/img/gpu/a100.png){:style="width: 60%; display:block; margin-left:auto; margin-right:auto"} *(Source: https://github.com/huggingface/peft*

