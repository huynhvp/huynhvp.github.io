---
layout: post
title: Table Representation Learning with Transformer
date: 2023-08-05 00:09:00
description: 
tags: research
categories: NLP, Table_Representation_Learning, AI
---

---

Tabular data (aka. table) is one of the most prevalent data structures used to store and present information on the web or in industry. Table contains rich and meaningful structural and semantc information. Making table machine-understandable is necessary to facilitate a wide range of applications related to table such as table-based question answering (an example shown below), fact-checking, table indexing/search/retrieval or table content imputation. 


![](/assets/img/trl/tapas_1.png){:style="width: 80%; display:block; margin-left:auto; margin-right:auto"}

*(source: [TAPAS (ACL 2020)](https://arxiv.org/pdf/2004.02349.pdf))*.

The emergence of Transformer-based Large Language Models (LLM) has revolutionized various natural language understanding tasks. As the massive corpora used to pre-train LLMs contains most of free-form text, or programming code, many works investigate and extents the applicability of LLMs to structured data like table. This post gathers prominent transformer models for learning the neural representation of tabular data.

*A survey on this emerging topic can be found at [Transformers for Tabular Data Representation: A Survey of Models and Applications, TACL 2022](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00544/115239/Transformers-for-Tabular-Data-Representation-A).*

---

<b>Table of Contents</b>
* TOC []
{:toc}

###### <b>TAPEX: Table Pre-training via Learning a Neural SQL Executor (Liu, ICLR 2022) </b>

The intuition of TAPEX is that if a LM can perform reasoning over a table via SQL queries, then it should have deep understanding of the table. To this end, TAPEX continue to pre-train a encoder-decoder LM (i.e. BART) on synthetic SQL-table queries with controlled quality/diversity, following the prevalent text-to-text framework:
- input: linearized SQL queries + linearized table ($$[HEAD], header_1,...,header_N, [ROW], 1, cell_{11},..., cell_{1N}$$)
- output: answer for the input query.

![](/assets/img/trl/tapex_1.png){:style="width: 50%; display:block; margin-left:auto; margin-right:auto"}

(source: copied from the paper).

With the versatile encoder-decoder architecture, TAPEX is easily fine-tuned to adapt to numerous downstream tasks by framing the tasks as text-to-text problems.

###### <b>TABBIE: Pretrained Representations of Tabular Data (Lida, NAACL 2021) </b>

TABBIE hypothizes that if a LM can detect a corrupted cell text, it may have the capacity to understand table structure: row/column seperators or cell boundaries. Without assuming the existence of other information than the table itself, TABBIE corrupts several cells in the table and train the LM to classifier whether every cell has been corrupted or not (similar to ELECTRA's discriminator). TABBIE employs two seperate Transformer to encode rows (*row* transformer) and columns (*column* transformer). The representation of cell $$c_{i,j}$$ is an average of the row $$i$$ embedding and column $$j$$ embedding. The cell embedding is initialized by the embedding of the cell text produced by a freezed BERT, added to two learnable positional embeddings (row and column).

![](/assets/img/trl/tabbie.png){:style="width: 50%; display:block; margin-left:auto; margin-right:auto"}

(source: copied from the paper).

###### <b>TABERT: Pretraining for Joint Understanding of Textual and Tabular Data (Yin, ACL 2020) </b>

TABERT creates a dataset including tables and surrounding text from Wikipedia pages, then jointly pre-train a LM on this mix of free-from text and structured tabular data. Due to limited input length, the table is cut off, only keep rows that are most relevant to the surrounding text (aka. *utterance*). Each table row is linearized (i.e. a cell is reprensented by $$Column\_Name \; \vert \; Column\_Type \; \vert \; Cell\_Value$$) and concatenated with the *utterance*, then is fed into BERT to yeild row-level embeddings of cell tokens and *utterance* tokens. Cell embeddings in the same column are aligned by vertical self-attention layers to allow for information flow across cell representations of different rows. The column representation is the average of embeddings of belonging cells.

![](/assets/img/trl/tabert.png){:style="width: 60%; display:block; margin-left:auto; margin-right:auto"}

(source: copied from the paper).

TABERT employs two unsupervised learning objectives:
- Masked Column Prediction: mask and predict column header and data type of the belonging cells.
- Cell Value Recovery: mask and predict the cell text.

###### <b>TAPAS: Weakly Supervised Table Parsing via Pre-training (Herzig, ACL 2020) </b>

TAPAS linearizes the concatenation of the utterance (e.g. query/question) and the table, then feeds it into a BERT model, performing unsupervised pre-training with typical masked LM objective. Each token embedding in the input is appended with additional special embedding: position emb, segment emb (whether token is within the utterance or table), column/row emb, rank emb (whether the cell text is string or float, if float, this emb encodes its order).

![](/assets/img/trl/tapas_2.png){:style="width: 70%; display:block; margin-left:auto; margin-right:auto"}

(source: copied from the paper).

###### <b>TURL: Table Understanding through Representation Learning (Deng, VLDB 2020) </b>

TURL continues to pre-train a Tiny-BERT on the linearization of [table caption, table topic, table headers, table cells]. Tabel cells are represented by a fusion of token embeddings of cell text and the learnable embedding of the associated entity. Positional embeddings are added to tokens in the caption and the header. Additional type embeddings are used to indicate whether a token is within the caption, the header or is a subject cell or object cell. The attention meachnism is modified in the way that not all tokens in the input are visible to each other (e.g. two tokens residing in different rows and different columns can not see each other). This design models the row-column interaction in the table. 

![](/assets/img/trl/turl_1.png){:style="width: 60%; display:block; margin-left:auto; margin-right:auto"}

![](/assets/img/trl/turl_2.png){:style="width: 60%; display:block; margin-left:auto; margin-right:auto"}

(source: copied from the paper).

Two unsupervised learning objectives in TURL:
- MLM: mask and predict a token in caption or header.
- Masked Entity Recovery: mask a cell and predict the entity representing it (i.e. entity classification).