---
layout: post
title: Recap of The Very Large Database (VLDB) Conference 2023
date: 2023-09-01 10:09:00
description: 
tags: research
categories: NLP, Database
---

---

![](/assets/img/vldb/IMG_1013.jpg){:style="width: 40%; height: 40%; display:block; margin-left:auto; margin-right:auto"} 
*(photo taken from Capilano Regional Park, North Vancouver, a highly recommended destination for nature lover)*

Last week, I had the opportunity to attend in-person The Very Large Data Base (VLDB) conference, held in Vancouver, Canada, from 28 August to 1 September. This year, the conference brought together over 1000 researchers, practitioners from the Database and NLP communities. With the rapid adoption of Large Language Model (LLM) in multiple fields, discussions on LLM, applications and impacts of LLM on data management drew a substantial audience. This post is a recap of presentation/discussion sessions that I was able to attend, focusing on topics that align with my research interests.

---

<b>Table of Contents</b>
* TOC []
{:toc}


### <b>VLDB by numbers</b>

![](/assets/img/vldb/IMG_0981.jpg){:style="width: 30%;"} ![](/assets/img/vldb/IMG_0982.jpg){:style="width: 30%;"} ![](/assets/img/vldb/IMG_0983.jpg){:style="width: 30%;"} 

- <b>1066</b> attendees, half of them are from USA/Canada.
- <b>1074</b> papers submitted to the main research track. <b>266</b> accepted paper.
- <b>16.2%</b> of papers are on Machine Learning, AI. (one of two hottest topics, together with Database Engines)
- <b>3</b> keynotes, one from a NLP rockstar [Yejin Choi](https://homes.cs.washington.edu/~yejin/) from AI2 (personally, I refresh her scholar profile once per month and read many of her papers).
- <b>15</b> workshops, one focused on [LLM + Database](https://haixun.github.io/llmdb/), one focused on [Tabular data](https://tabular-data-analysis.github.io/tada2023/).
- Google, Microsoft, Amazon, Salesforce, Huawei, etc have stands at the conference, to showcase their products as well as present job opportunities. 

### <b>NLP community meets Database community</b>
#### Common Sense: the Dark Matter of Language and Intelligence - Yejin Choi's keynote. 

She gave an insightful talk on the subject: <b>LLMs (e.g. ChatGPT, GPT4) are incredibly powerful, yet surprisingly brittle</b>, makes nonsensical errors or inconsistent answers (e.g. question + negated question get the same answer). She argues this is due to the lack of common sense knowledge (examples of common sense: bird can fly or it's not ok to keep the fridge door open). While scale people believes this issue could be (easily) fixed when model gets bigger and bigger and consumes more similar training data, she questions why we even need to take that approach when children can naturally acquire such common sense knowledge without reading trillion words. <b>During the keynote, she presented methods/algorithms (as described below) that help build smaller but competitive models, powered with knowledge ("knowledge model"), compared to extreme-scale language models</b>:
- [Symbolic Knowledge Distillation](https://aclanthology.org/2022.naacl-main.341.pdf): manually crafting a high-quality common sense knowledge graph to teach a common sense model is expensive, hard to scale, hence, resulting a knowledge repository with limited coverage. Based on the intuition that a LLM (e.g. GPT-3) contains a vast amount of knowledge, but may be noisy and not fully exploitable, [West et al](https://aclanthology.org/2022.naacl-main.341.pdf) propose to distill high-quality commonsense knowledge from this LLM (referred to as the teacher model) through prompting (e.g. "X goes jogging. Prerequisites: For this to happen,____") and filtering the prompt's results by a small critic model. The critic model is fine-tuned on a set of *correct vs. incorrect* human judgements on a randomly sampled set of knowledge extracted (but unfiltered) from GPT-3. As a result, for the first time, they obtain <b>ATOMIC</b>, a commonsense KG, automatically distilled from GPT-3, outperforms human-curated KG in three criteria: quantity, quality and diversity. 

  ![](/assets/img/vldb/distill.png){:style="width: 30%;"} *(source: copied from the paper).*

  Subsequently, the distilled commensense KG is employed to train a much smaller model (GPT-2 XL), resulting a knowledge model, <b>COMET_DISTIL</b>, surpassing the commonsense of GPT-3.

- Inference-time algorithms: enhancing the common sense capability of LM at inference-time by improving prompting technique or decoding technique, requiring no further fine-tuning.
  - [Constrained NeuroLogic decoding](https://arxiv.org/pdf/2212.09246.pdf): while common sense statements are simple, clear and short, text generated by small LM can be often trivially long or repetitive. To improve the generation quality, NeuroLogic Decoding enforces logical constraints at decoding time (e.g. limiting number of function words such as "in", "on", or excluding connective words such as "although", "since", or a given word must be generated).

    ![](/assets/img/vldb/i2d2.png){:style="width: 60%;"}
    *(source: copied from the paper).*

  - [Maieutic Prompting](https://aclanthology.org/2022.emnlp-main.82.pdf): LLM can generate inconsistent and unreliable explanation when a question and its negated version get the same answer (e.g. "One is a number that comes before zero ? ... True" vs. "One is a number that comes after zero ? ... True"). They introduce a novel prompting technique, <b> Maieutic Prompting</b> to improve the consistency in LLM's generation. Inspired by Socratic style of conversation, the inference process exploits the depth of reasoning by asking recursively if a newly generated explanation is logically consistent with its parent (previous) explanation, as illustrated in the figure below: 

    ![](/assets/img/vldb/maieutic.png){:style="width: 50%;"}
    *(source: copied from the paper).*

#### Language Model Agents for Building Natural Language Interfaces to Data - [Tao Yu](https://taoyds.github.io/)'s keynote, XLANG NLP Lab, University of Hong Kong, in Databases and Large Language Models (LLMDB) workshop. 

Teaching LLM to use tools (<b>Tools-Augmented LLM</b>) is probably one of the most exciting capabilities of LLM, enabling it to interact with the real world and address some of limitations:

- Math capabilities (by calling a calculator).
- Keep LLM up-to-date with the latest information of real world (by coupling with a search engine).
- Enhance Interoperability and Trustworthiness (by tracing tool's operations, or citing sources).

  ![](/assets/img/vldb/lm_agent.png){:style="width: 50%;"}
  *(source: copied from author's slides).*

XLANG Lab is building such LM Agent, supporting a wide range of data-related tools (Python, SQL, Plot tools, Kaggle tools...). Additionally, as tools involve both text and code, they introduces <b>Lemur-70B</b>, built on top of LLaMa-2, balancing text and code capabilities.

  ![](/assets/img/vldb/xlang.png){:style="width: 40%;"} ![](/assets/img/vldb/lemur.png){:style="width: 30%;"}
  *(source: copied from author's slides).*

### <b>Database Community in the era of LLM</b>

#### Panel: Will LLMs reshape, supercharge, or kill data science ?

The panel, moderated by Alon Halevey (Meta), featured Yejin Choi (University of Washington and AI2), Avrilia Floratou (Microsoft), Michael Franklin (University of Chicago), Natasha Noy (Google) and Haixun Wang (Instacart).

To kick off the discussion, Alon Halevey showcased two examples that GPT-4 performs very well:
- He asks GPT-4 to read an uploaded .csv file containing the books that he has ever read and perform several tasks such as: count number of books, deduplicate entries, plot a histogram. He then asks if GPT-4 can suggest some creative new tasks based on that .csv file. GPT-4 did well (although I don't recall the details)
- He extracted posts from his friend on facebook over a span of 2 months. He asks GPT-4 to summarize these posts in a structured way by defining a schema and generating attribute values. The results provided by GPT-4 were awesome.

Given the fact that LLMs has been significantly advancing several long-standing challenges that the database community has ben tackling for decades, Alon asks the panel following questions:

  ![](/assets/img/vldb/panel_1.jpeg){:style="width: 40%;"} (from Halevy) ![](/assets/img/vldb/panel_2.jpeg){:style="width: 40%;"} (from Floratou)

Several thoughts from the panel list:
- Writing manually SQL stuffs will go way.
- LLM changes remarkably the search engine (understand the intent and give good answer). Past trends go from unstructured data to structured data via ETL, Wrangling. Present trends: structured data --> un-structured data with LLM.
- LLM has issue with latency, data freshness.
- LLM has no exact durable memory, while database does.
- Replacing .csv, .yaml by .txt is challenging.
- Add whole database on-the-fly is expensive --> Tools-Augmented LLMs.

#### The first edition of [Databases and Large Language Models workshop](Databases and Large Language Models)

This workshop has emerged to meet the "urgent and exiting" need of integrating the impressive capabilities of LLMs into the real-world data management applications.

  - <b>Wang-Chiew Tan (Meta)'s keynote</b>: Using LLMs to Query Unstructured and Structured Data (similar to Alon Halevey (Meta)'s keynote at [Tabular Data Analysis Workshop](https://tabular-data-analysis.github.io))

      Personal data records valuable information (e.g. health, activities, hobby, etc) throughout one's life. 

      ![](/assets/img/vldb/meta_1.png){:style="width: 40%; display:block; margin-left:auto; margin-right:auto"}

      *(source: copied from the slides)*.

      This talk presents interesting opportunities of leveraging LLMs to interact with our personal timeline data (whether structured or not), thereby providing new user experience. As LLMs know nothing about us, there are two ways to query timeline data in natural language using LLMs:
      - Retrieval Augmented LM (RAG) (Figure a): LM consults an external repository that stores personal data to answer the question. <b>The speaker argues that this scheme does not work well on complex/multi-hop questions</b>.

          ![](/assets/img/vldb/meta_2.png){:style="width: 30%"} 
          *(source: copied from the slides)*.

      - Tool Augmented LM (Figure b): LM converts the question into SQL and call a SQl-engine to execute the SQL over database of personal data.

          ![](/assets/img/vldb/meta_3.png){:style="width: 40%"} ![](/assets/img/vldb/meta_4.png){:style="width: 40%"} 
          *(source: copied from the slides)*.

  - <b>Laurel Orr (Numbers Station)'s keynote</b>: Deploying LLMs on Structured Data Tasks: Lessons from the Trenches

    She talked about her experiences when it comes to deploying LLMs applications for Structured data Wrangling in production, at [Numbers Station](https://www.numbersstation.ai/).

      ![](/assets/img/vldb/number_3.jpeg){:style="width: 30%"} 
      *(source: copied from the slides)*.   

    In a nutshell, she frames "LLMs for Data Wrangling in Production" as: "Possible, Exciting, Not Easy", illustrated by three challenges:
    - (a) - High Cost: real relational database is huge, put it all in the context is tricky. Solution: Tools-augmented LLMs use tools to interact with database.
    - (b) - Lacking Personalization: General LLMs lack required enterprise knowledge. Solution: fine-tune LLMs with enterprise data.
    - (c) - Not enough context.

      (a) ![](/assets/img/vldb/number_1.jpeg){:style="width: 30%"} (b) ![](/assets/img/vldb/number_2.jpeg){:style="width: 30%"} (c) ![](/assets/img/vldb/IMG_1030.jpeg){:style="width: 30%"} 

      *(source: copied from the slides)*.    

#### Knowledge Graph in the era of LLM

Xin Luna Dong (Meta) was awarded the 2023 VLDB Women in Database Research Award for her significant contributions to knowledge graph construction and data integration. You can refer to her [vision](https://arxiv.org/pdf/2308.14217.pdf) on the next generations of KG, leveraging the recent big success of LLMs.

#### Several paper sessions

- [How Large Language Models Will Disrupt Data Management](https://www.vldb.org/pvldb/vol16/p3302-fernandez.pdf) (University of Chicago):

    ![](/assets/img/vldb/IMG_0993.jpg){:style="width: 30%"} 
    *(source: copied from the slides)*.   

- [Can Foundation Models Wrangle Your Data?](https://www.vldb.org/pvldb/vol16/p738-narayan.pdf) (Stanford): LLMs show strong zero-shot and few-shot capability in data wrangling tasks such as entity matching, error detection and data imputation. A benchmark for this topic will soon be integrated into the famous [LLM HELM benchmark](https://crfm.stanford.edu/helm/latest/).
- [CatSQL: Towards Real World Natural Language to SQL Applications](https://www.vldb.org/pvldb/vol16/p1534-fu.pdf) (Alibaba Group): new state-of-the-art natural language to SQL converter on Spider benchmark.

### <b>Tabular Data (table) is gaining attention</b>

- Together with [Table Representation Learning Workshop](https://table-representation-learning.github.io), co-located with Neurips 2023 and [Semantic Web Challenge on Tabular data to Knowledge KG Matching](https://sem-tab-challenge.github.io/2023/), co-located with ISWC 2023, this year's VLDB conference organized [Tabular Data Analysis Workshop](https://tabular-data-analysis.github.io). The workshop featured two keynote speakers: (i) Renée Miller (Northeastern University) on Table Discovery and Integration from Data Lake of tables and (ii) Alon Halevey (Meta) on the potential of LLM of interfacing a database with natural language query. 

- As part of Renée Miller's keynote, she presented two papers accepted at VLDB:
    - [Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning](https://arxiv.org/pdf/2210.01922.pdf): <b>table union search</b> involves searching for tables in a data lake that are semantically close to a given table by evaluating the similarity between column embeddings in two tables. They employ the popular contrastive self-supervised training to learn the embeddings of table columns. The key point lies in data augmentation for the training: given a input table $$x$$, applying transformation operators $$f$$ such as cell dropping, cell swapping, row shuffling to $$x$$ yields a positive training sample ($$x$$, $$f(x)$$).

    ![](/assets/img/vldb/alite_1.png){:style="width: 60%; display:block; margin-left:auto; margin-right:auto"}

    *(source: copied from the paper)*.

    - [Integrating Data Lake Tables](http://vldb.org/pvldb/vol16/p932-khatiwada.pdf): presents different techniques for integrating relevant tables discovered from data lake into a single table.

    ![](/assets/img/vldb/alite_2.png){:style="width: 50%; display:block; margin-left:auto; margin-right:auto"}

    *(source: copied from the paper)*.

    Her talks attracted a lot of discussion among researchers in the Semantic Web community. They agreed that adding a semantic layer on top of table (via Semantic Table Interpretation) would facilitate and improve the discovery and integration of data lake tables. Several related papers have been presented during the conference:
    - [RECA: Related Tables Enhanced Column Semantic Type Annotation Framework](https://www.vldb.org/pvldb/vol16/p1319-sun.pdf) (The University of Hong Kong).
    - [Column Type Annotation using ChatGPT](https://ceur-ws.org/Vol-3462/TADA1.pdf) (University of Mannheim).
    - [Towards Generative Semantic Table Interpretation](https://ceur-ws.org/Vol-3462/TADA7.pdf) (Orange + EURECOM).
    - 
  
- <b>VLDB Best paper award</b>: [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://www.vldb.org/pvldb/vol16/p3391-he.pdf) (Georgia Tech & Microsoft): 

    Non-relational tables, despite of being overwhelming on the wild web, are not easy to be queried using SQL-based tools. Transforming non-relational tables into standard relational table (Figure b) is a non-trivial task and has been a longstanding challenge in the database community (Figure a).

    (a) ![](/assets/img/vldb/auto_table_1.png){:style="width: 30%"} (b) ![](/assets/img/vldb/auto_table_2.png){:style="width: 60%"} 

    *(source: copied from the paper)*.

    This work proposes <b>Auto-Tables</b>, a pipeline for transforming non-relational tables into standard relational table automatically. The process involves the use table transformation operators such as transpose, stacking, splitting, etc to generate a self-supervised training dataset which will be used to train a deep neural network.